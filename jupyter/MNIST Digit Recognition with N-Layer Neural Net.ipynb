{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognition with N-Layer Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the MNIST data set\n",
    "import cloudpickle, gzip, numpy\n",
    "\n",
    "f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cloudpickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        np.random.seed()\n",
    "        self.weights = []\n",
    "        self.hl_sizes = []\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def d_sigmoid_output(self, out):\n",
    "        return out * (1 - out)\n",
    "    \n",
    "    def test(self,X,Y):\n",
    "        # initialize layers\n",
    "        layers = []\n",
    "        \n",
    "        # how many training examples are provided?\n",
    "        num_inputs = X.shape[0]\n",
    "        \n",
    "        # add a bias node with a value of 1 to\n",
    "        # the input layer and hidden layers\n",
    "        X = np.append(X,np.ones((num_inputs,1)),axis=1)\n",
    "        \n",
    "        # load examples into input layer\n",
    "        layers.append(X)\n",
    "        \n",
    "        # feed forward to all hidden layers\n",
    "        for j in range(len(self.hl_sizes)):\n",
    "            # compute node values\n",
    "            layer = self.sigmoid(np.dot(layers[j], self.weights[j]))\n",
    "        \n",
    "            # reset bias node\n",
    "            layer[:,layer.shape[1]-1] = 1\n",
    "            layers.append(layer)\n",
    "\n",
    "        # feed forward to output layer\n",
    "        layers.append(self.sigmoid(np.dot(layers[j+1], self.weights[j+1])))\n",
    "        \n",
    "        return layers[len(layers)-1]\n",
    "        \n",
    "    def train(self, X=np.array([]), Y=np.array([]), iterations=1, alpha=1, bias=1, batch_size=0, hl_sizes=[4], dropout=False, dropout_ratio=0.5):\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise Exception(\"You must specify both X and Y\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception(\"Number of rows in X and Y must be the same: %d != %d\" % (X.shape[0],Y.shape[0]))\n",
    "        \n",
    "        # set hidden layer sizes\n",
    "        self.hl_sizes = hl_sizes\n",
    "        \n",
    "        # how many training examples are provided?\n",
    "        num_inputs = X.shape[0]\n",
    "        \n",
    "        # how many nodes do we need for then input layer?\n",
    "        input_vector_length = X.shape[1]\n",
    "        \n",
    "        # if batch_size is not provided (i.e. is set to zero by default),\n",
    "        # then default to full batch for gradient descent\n",
    "        if batch_size <= 0:\n",
    "            batch_size = num_inputs\n",
    "        \n",
    "        # add a bias node with a value of 1 to\n",
    "        # the input layer and hidden layers\n",
    "        X = np.append(X,np.ones((num_inputs,1)),axis=1)\n",
    "        \n",
    "        # randomly initialize weights between -1 and 1\n",
    "        # for input_layer -> h1_layer\n",
    "        self.weights = []\n",
    "        self.weights.append(2*np.random.random((input_vector_length + 1, self.hl_sizes[0] + 1)) - 1)\n",
    "        \n",
    "        # randomly initialize weights between -1 and 1\n",
    "        # between all hidden layers\n",
    "        if len(self.hl_sizes) > 1:\n",
    "            for j in range(1,len(self.hl_sizes)):\n",
    "                self.weights.append(2*np.random.random((hl_sizes[j-1] + 1, self.hl_sizes[j] + 1)) - 1)\n",
    "        \n",
    "        # randomly initialize weights between -1 and 1\n",
    "        # for last hidden layer -> output_layer\n",
    "        self.weights.append(2*np.random.random((self.hl_sizes[len(self.hl_sizes)-1] + 1,Y.shape[1])) - 1)\n",
    "        \n",
    "        # initialize layers\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            batch_start = 0\n",
    "            \n",
    "            layers = []\n",
    "            weight_updates = []\n",
    "            \n",
    "            for j in range(len(self.weights)):\n",
    "                weight_updates.append(np.zeros_like(self.weights[j]))\n",
    "            \n",
    "            total_avg_training_error = 0\n",
    "            \n",
    "            # --------------------------------------------------------\n",
    "            # TRAIN IN MINI BATCHES\n",
    "            # (or full batch if no batch_size was provided)\n",
    "            #\n",
    "            # TODO: This current implementation isn't very useful\n",
    "            #       because it still processes batches serially.\n",
    "            #       The whole point of mini-batches is so you can\n",
    "            #       parallelize the gradient decent during backpropagation.\n",
    "            #       So, the next iteration of this code should take \n",
    "            #       adcantage of threading to actually parallelize the\n",
    "            #       crunching of the mini-batches \n",
    "            # --------------------------------------------------------\n",
    "            \n",
    "            while batch_start <= (num_inputs - 1):\n",
    "                \n",
    "                batch_end = batch_start + batch_size\n",
    "                \n",
    "                x = X[batch_start : batch_end]\n",
    "                y = Y[batch_start : batch_end]\n",
    "                \n",
    "                # TODO: make batch selection random so each iteration\n",
    "                #       isn't always seeing the same mini-batches\n",
    "\n",
    "                # set input layer equal to the examples in the mini-batch\n",
    "                layers.append(x)\n",
    "                \n",
    "                # -------------------------------\n",
    "                # FEED FORWARD\n",
    "                # -------------------------------\n",
    "                \n",
    "                # feed forward to all hidden layers\n",
    "                for j in range(len(self.hl_sizes)):\n",
    "                    # compute node values\n",
    "                    layer = self.sigmoid(np.dot(layers[j], self.weights[j]))\n",
    "                    \n",
    "                    # perform dropout if requested\n",
    "                    if dropout == True:\n",
    "                        \n",
    "                        # generate an array of 1's and 0's\n",
    "                        # drawn from a binomial distribution with probability of dropout_ratio\n",
    "                        dropouts = np.random.binomial(1, 1 - dropout_ratio, (layer.shape[0],layer.shape[1]))\n",
    "                                           \n",
    "                        # multiply the node values in the layer by the randomly assigned \n",
    "                        # values in dropouts, effectively \"turning off\" some nodes (they get multiplied by 0)\n",
    "                        layer = layer * dropouts\n",
    "                    \n",
    "                    # reset bias node\n",
    "                    layer[:,layer.shape[1]-1] = 1\n",
    "                    layers.append(layer)\n",
    "                    \n",
    "                    # reset bias node in the layer that was just computed\n",
    "                    for k in range(batch_size):\n",
    "                        try:\n",
    "                            layers[j+1][k][self.hl_sizes[j]] = 1\n",
    "                        except IndexError:\n",
    "                            break\n",
    "                \n",
    "                # reset output layer\n",
    "                layers.append(np.zeros((x.shape[0],10)))\n",
    "                    \n",
    "                # feed forward to output layer\n",
    "                layers[len(layers)-1] = self.sigmoid(np.dot(layers[j+1], self.weights[j+1]))\n",
    "                \n",
    "                # measure error\n",
    "                training_error = layers[len(layers)-1] - y\n",
    "                total_avg_training_error += np.mean(np.abs(training_error))\n",
    "                \n",
    "                # -------------------------------\n",
    "                # BACKPROPAGATE\n",
    "                # -------------------------------\n",
    "                \n",
    "                deltas = []\n",
    "                errors = []\n",
    "\n",
    "                errors.insert(0, training_error)\n",
    "                deltas.insert(0, training_error * self.d_sigmoid_output(layers[len(layers)-1]))\n",
    "                \n",
    "                # continue backwards, calculating the error contribution from each layer\n",
    "                # and an appropriate delta for determining weight updates\n",
    "                for j in reversed(range(0,len(self.hl_sizes))):\n",
    "                    errors.insert(0, np.dot(deltas[0],self.weights[j+1].T))\n",
    "                    deltas.insert(0, errors[0] * self.d_sigmoid_output(layers[j+1]))\n",
    "            \n",
    "                # calculate weight updates       \n",
    "                for j in range(0,len(self.weights)):\n",
    "                    weight_updates[j] += np.dot(layers[j].T, deltas[j])\n",
    "                \n",
    "                # figure out what index to start grabbing our next batch from\n",
    "                batch_start += batch_size\n",
    "            \n",
    "            # At this point, the batch has been processed, and all the updates\n",
    "            # for the weights have been calculated\n",
    "            \n",
    "            # update weights\n",
    "            for j in range(len(self.weights)-1):\n",
    "                self.weights[j] -= (alpha * weight_updates[j])\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(\"Iteration: %d Error: %.5f\" % (i,total_avg_training_error))\n",
    "        \n",
    "        print(\"Iteration: %d Error: %.5f\" % (i,total_avg_training_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = train_set[0][:1000]\n",
    "_Y = train_set[1][:1000]\n",
    "Y = np.empty((len(_Y),10))\n",
    "\n",
    "print(len(train_set[0]))\n",
    "\n",
    "for idx, y in enumerate(_Y):\n",
    "    if y == 0:\n",
    "         Y[idx] = [1,0,0,0,0,0,0,0,0,0]\n",
    "    elif y == 1:\n",
    "        Y[idx] = [0,1,0,0,0,0,0,0,0,0]\n",
    "    elif y == 2:\n",
    "        Y[idx] = [0,0,1,0,0,0,0,0,0,0]\n",
    "    elif y == 3:\n",
    "        Y[idx] = [0,0,0,1,0,0,0,0,0,0]\n",
    "    elif y == 4:\n",
    "        Y[idx] = [0,0,0,0,1,0,0,0,0,0]\n",
    "    elif y == 5:\n",
    "        Y[idx] = [0,0,0,0,0,1,0,0,0,0]\n",
    "    elif y == 6:\n",
    "        Y[idx] = [0,0,0,0,0,0,1,0,0,0]\n",
    "    elif y == 7:\n",
    "        Y[idx] = [0,0,0,0,0,0,0,1,0,0]\n",
    "    elif y == 8:\n",
    "        Y[idx] = [0,0,0,0,0,0,0,0,1,0]\n",
    "    elif y == 9:\n",
    "        Y[idx] = [0,0,0,0,0,0,0,0,0,1]\n",
    "        \n",
    "\n",
    "input_size = len(X[0])\n",
    "\n",
    "net = NN()\n",
    "net.train(X=X,Y=Y,iterations=10000,hl_sizes=[input_size*2, input_size/2], alpha=0.01, dropout=True, dropout_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8684/10000 correct\n",
      "Test accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "X = valid_set[0]\n",
    "Y = valid_set[1]\n",
    "\n",
    "results = net.test(X,Y)\n",
    "_results = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    \n",
    "    for j,el in enumerate(result):\n",
    "        result[j] = int(el+0.5)\n",
    "    \n",
    "    if np.array_equal(result,[1,0,0,0,0,0,0,0,0,0]):\n",
    "        _results.append(0)\n",
    "    elif np.array_equal(result,[0,1,0,0,0,0,0,0,0,0]):\n",
    "        _results.append(1)\n",
    "    elif np.array_equal(result,[0,0,1,0,0,0,0,0,0,0]):\n",
    "        _results.append(2)\n",
    "    elif np.array_equal(result,[0,0,0,1,0,0,0,0,0,0]):\n",
    "        _results.append(3)\n",
    "    elif np.array_equal(result,[0,0,0,0,1,0,0,0,0,0]):\n",
    "        _results.append(4)\n",
    "    elif np.array_equal(result,[0,0,0,0,0,1,0,0,0,0]):\n",
    "        _results.append(5)\n",
    "    elif np.array_equal(result,[0,0,0,0,0,0,1,0,0,0]):\n",
    "        _results.append(6)\n",
    "    elif np.array_equal(result,[0,0,0,0,0,0,0,1,0,0]):\n",
    "        _results.append(7)\n",
    "    elif np.array_equal(result,[0,0,0,0,0,0,0,0,1,0]):\n",
    "        _results.append(8)\n",
    "    elif np.array_equal(result,[0,0,0,0,0,0,0,0,0,1]):\n",
    "        _results.append(9)\n",
    "    else:\n",
    "        _results.append(\"n/a\")\n",
    "\n",
    "correct = 0\n",
    "for i, y in enumerate(Y):\n",
    "    if y == _results[i]:\n",
    "        correct += 1\n",
    "        \n",
    "print(\"%d/%d correct\" % (correct,len(X)))\n",
    "print(\"Test accuracy: %.2f\" % (correct/len(X)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
