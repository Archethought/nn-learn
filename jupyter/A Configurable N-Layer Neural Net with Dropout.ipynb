{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Configurable N-Layer Neural Net with Dropout\n",
    "\n",
    "With this Neural Net class, you can:\n",
    "\n",
    "* Choose the number and size of your hidden layers\n",
    "* Choose an alpha (a.k.a. step size, learning rate, defaults to 1)\n",
    "* Choose a bias (defaults to 1)\n",
    "* Choose the number of training iterations (defaults to 1, but you probably want 10,000+)\n",
    "* Enable dropout, and choose a dropout ratio\n",
    "\n",
    "The output layer is single node.\n",
    "The arguments in the running example at the end of the notebook build a neural net with 4 hidden layers. Not necessarily because that's optimal, but just to show how a basic deep neural net can be built with this class.\n",
    "\n",
    "The provide example learns this concept given a vector `x = <x1, x2, x3, x4>` where each element is `0` or `1`.\n",
    "\n",
    "    (x0 and x1) or (x2 and x3)\n",
    "    \n",
    "...and outputs 0 for false, 1 for true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        np.random.seed()\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def d_sigmoid_output(self, out):\n",
    "        return out * (1 - out)\n",
    "    \n",
    "    def train(self, X=np.array([]), Y=np.array([]), iterations=1, alpha=1, bias=1, batch_size=0, hl_sizes=[4], dropout=False, dropout_ratio=0.5):\n",
    "        if X.size == 0 or Y.size == 0:\n",
    "            raise Exception(\"You must specify both X and Y\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception(\"Number of rows in X and Y must be the same: %d != %d\" % (X.shape[0],Y.shape[0]))\n",
    "        \n",
    "        # how many training examples are provided?\n",
    "        num_inputs = X.shape[0]\n",
    "        \n",
    "        # how many nodes do we need for then input layer?\n",
    "        input_vector_length = X.shape[1]\n",
    "        \n",
    "        # if batch_size is not provided (i.e. is set to zero by default),\n",
    "        # then default to full batch for gradient descent\n",
    "        if batch_size <= 0:\n",
    "            batch_size = num_inputs\n",
    "        \n",
    "        # add a bias node with a value of 1 to\n",
    "        # the input layer and hidden layers\n",
    "        X = np.append(X,np.ones((num_inputs,1)),axis=1)\n",
    "        \n",
    "        # randomly initialize weights between -1 and 1\n",
    "        # for input_layer -> h1_layer\n",
    "        weights = []\n",
    "        weights.append(2*np.random.random((input_vector_length + 1, hl_sizes[0] + 1)) - 1)\n",
    "        \n",
    "        # randomly initialize weights between -1 and 1\n",
    "        # between all hidden layers\n",
    "        if len(hl_sizes) > 1:\n",
    "            for j in range(1,len(hl_sizes)):\n",
    "                weights.append(2*np.random.random((hl_sizes[j-1] + 1, hl_sizes[j] + 1)) - 1)\n",
    "        \n",
    "        # randomly initialize weights between -1 and 1\n",
    "        # for last hidden layer -> output_layer\n",
    "        weights.append(2*np.random.random((hl_sizes[len(hl_sizes)-1] + 1,1)) - 1)\n",
    "            \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            batch_start = 0\n",
    "            \n",
    "            weight_updates = []\n",
    "            for j in range(len(weights)):\n",
    "                weight_updates.append(np.zeros_like(weights[j]))\n",
    "            \n",
    "            total_avg_training_error = 0\n",
    "            \n",
    "            # --------------------------------------------------------\n",
    "            # TRAIN IN MINI BATCHES\n",
    "            # (or full batch if no batch_size was provided)\n",
    "            #\n",
    "            # TODO: This current implementation isn't very useful\n",
    "            #       because it still processes batches serially.\n",
    "            #       The whole point of mini-batches is so you can\n",
    "            #       parallelize the gradient decent during backpropagation.\n",
    "            #       So, the next iteration of this code should take \n",
    "            #       adcantage of threading to actually parallelize the\n",
    "            #       crunching of the mini-batches \n",
    "            # --------------------------------------------------------\n",
    "            \n",
    "            while batch_start <= (num_inputs - 1):\n",
    "                \n",
    "                batch_end = batch_start + batch_size\n",
    "                \n",
    "                x = X[batch_start : batch_end]\n",
    "                y = Y[batch_start : batch_end]\n",
    "                \n",
    "                # TODO: make batch selection random so each iteration\n",
    "                #       isn't always seeing the same mini-batches\n",
    "                \n",
    "                # initialize layers\n",
    "                layers = []\n",
    "\n",
    "                # set input layer equal to the examples in the mini-batch\n",
    "                layers.append(x)\n",
    "                \n",
    "                # -------------------------------\n",
    "                # FEED FORWARD\n",
    "                # -------------------------------\n",
    "                \n",
    "                # feed forward to all hidden layers\n",
    "                for j in range(len(hl_sizes)):\n",
    "                    # compute node values\n",
    "                    layer = self.sigmoid(np.dot(layers[j], weights[j]))\n",
    "                    \n",
    "                    # perform dropout if requested\n",
    "                    if dropout == True:\n",
    "                        \n",
    "                        # generate an array of 1's and 0's\n",
    "                        # drawn from a binomial distribution with probability of dropout_ratio\n",
    "                        dropouts = np.random.binomial(1, 1 - dropout_ratio, (layer.shape[0],layer.shape[1]))\n",
    "                                           \n",
    "                        # multiply the node values in the layer by the randomly assigned \n",
    "                        # values in dropouts, effectively \"turning off\" some nodes (they get multiplied by 0)\n",
    "                        layer = layer * dropouts\n",
    "                    \n",
    "                    # reset bias node\n",
    "                    layer[:,layer.shape[1]-1] = 1\n",
    "                    layers.append(layer)\n",
    "                    \n",
    "                    # reset bias node in the layer that was just computed\n",
    "                    for k in range(batch_size):\n",
    "                        try:\n",
    "                            layers[j+1][k][hl_sizes[j]] = 1\n",
    "                        except IndexError:\n",
    "                            break\n",
    "                \n",
    "                # feed forward to output layer\n",
    "                layers.append(self.sigmoid(np.dot(layers[j+1], weights[j+1])))\n",
    "                \n",
    "                # measure error\n",
    "                training_error = layers[len(layers)-1] - y\n",
    "                total_avg_training_error += np.mean(np.abs(training_error))\n",
    "                \n",
    "                # -------------------------------\n",
    "                # BACKPROPAGATE\n",
    "                # -------------------------------\n",
    "                \n",
    "                deltas = []\n",
    "                errors = []\n",
    "\n",
    "                errors.insert(0, training_error)\n",
    "                deltas.insert(0, training_error * self.d_sigmoid_output(layers[len(layers)-1]))\n",
    "                \n",
    "                # continue backwards, calculating the error contribution from each layer\n",
    "                # and an appropriate delta for determining weight updates\n",
    "                for j in reversed(range(0,len(hl_sizes))):\n",
    "                    errors.insert(0, np.dot(deltas[0],weights[j+1].T))\n",
    "                    deltas.insert(0, errors[0] * self.d_sigmoid_output(layers[j+1]))\n",
    "            \n",
    "                # calculate weight updates       \n",
    "                for j in range(0,len(weights)):\n",
    "                    weight_updates[j] += np.dot(layers[j].T, deltas[j])\n",
    "                \n",
    "                # figure out what index to start grabbing our next batch from\n",
    "                batch_start += batch_size\n",
    "            \n",
    "            # At this point, the batch has been processed, and all the updates\n",
    "            # for the weights have been calculated\n",
    "            \n",
    "            # update weights\n",
    "            for j in range(len(weights)-1):\n",
    "                weights[j] -= (alpha * weight_updates[j])\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print(\"Iteration: %d Error: %.5f\" % (i,total_avg_training_error))\n",
    "        \n",
    "        print(\"Iteration: %d Error: %.5f\" % (i,total_avg_training_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,0,1],\n",
    "              [0,0,1,1],\n",
    "              [1,0,0,1],\n",
    "              [1,1,1,0],\n",
    "              [1,1,0,0],\n",
    "              [1,0,0,0],\n",
    "              [1,0,1,0],\n",
    "              [1,0,0,0]])\n",
    "\n",
    "Y = np.array([[1,1,0,1,1,1,0,1]]).T\n",
    "\n",
    "net = NN()\n",
    "net.train(X, Y, hl_sizes=[8,16,32,64], iterations=100000, batch_size=8, alpha=14, dropout=True, dropout_ratio=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
