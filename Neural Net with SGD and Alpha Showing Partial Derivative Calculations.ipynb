{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailing the Partial Derivative Calculations during Backpropagation\n",
    "\n",
    "This 3-Layer Nerual Net example calculates all the partial derivatives explicitly during the backpropagation phase of training.\n",
    "\n",
    "Credit to Matt Mazur for his excellent [step-by-step explanation](http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example) of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "    paths: {\n",
    "        flot: '//www.flotcharts.org/javascript/jquery.flot.min',\n",
    "        d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min'\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output {\n",
    "  max-height: 1000px;\n",
    "  overflow-y: scroll;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from IPython.display import Javascript\n",
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set alpha\n",
    "# ----\n",
    "# Alpha is the step size multiplier. Each time we adjust\n",
    "# weights, we'll multiply the adjustment times alpha, \n",
    "# scaling the size of the adjustment during each update\n",
    "alpha = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Activation function\n",
    "# ----\n",
    "# We'll use the sigmoid function as our activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Activation derivative\n",
    "# ----\n",
    "# We use the derivative to find the direction\n",
    "# of the gradient in SGD so we know which way (positive or negative)\n",
    "# to move the weights when we update them\n",
    "def sigmoid_output_derivative(output):\n",
    "    return output*(1 - output)\n",
    "\n",
    "# Inputs\n",
    "X = np.array([[0,0,1]])\n",
    "\n",
    "# Outputs\n",
    "Y = np.array([[0]]).T\n",
    "\n",
    "# Seed a random number generator\n",
    "np.random.seed(2)\n",
    "\n",
    "# Randomly initialize weights between -1 and 1\n",
    "W0 = 2*np.random.random((3,4)) - 1\n",
    "W1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "for i in range(60000):\n",
    "    \n",
    "    # ------------\n",
    "    # Feed forward\n",
    "    # ------------\n",
    "    \n",
    "    # Set Inputs\n",
    "    layer0 = X\n",
    "    \n",
    "    # For Layer 1, calculate:\n",
    "    #   layer1_inputs (individual inputs)\n",
    "    #   layer1_net_input (net input)\n",
    "    #   layer1_outputs (individual inputs)\n",
    "    #   layer1_net_output (net ouput) \n",
    "    layer1_inputs     = np.dot(layer0,W0)\n",
    "    layer1_outputs    = sigmoid(layer1_inputs)\n",
    "    \n",
    "    # Calculate input and output for all nodes in Layer 2\n",
    "    layer2_inputs     = np.dot(layer1_outputs,W1)\n",
    "    layer2_outputs    = sigmoid(layer2_inputs)\n",
    "    \n",
    "    # ---------------\n",
    "    # Backpropagation\n",
    "    # ---------------\n",
    "    \n",
    "    # First, we want to know how much a change in each of the W1 weights\n",
    "    # affects the total error. We have 4 weights connected to the output layer,\n",
    "    # so we will run this calculation for each weight. In partial derivative speak...\n",
    "    #\n",
    "    # Affect on ErrorT of changing W1[0] = d_ErrorT / d_W1[0]\n",
    "    # Affect on ErrorT of changing W1[1] = d_ErrorT / d_W1[1]\n",
    "    # Affect on ErrorT of changing W1[2] = d_ErrorT / d_W1[2]\n",
    "    # Affect on ErrorT of changing W1[3] = d_ErrorT / d_W1[3]\n",
    "    \n",
    "    # This is where the Chain Rule is used to calculate these partial derivatives.\n",
    "    # The Chain Rule tells us that:\n",
    "    #\n",
    "    # d_ErrorT / d_W1[0] = (d_ErrorT / d_layer2_outputs[0]) * \n",
    "    #                      (d_layer2_outputs[0] / d_layer2_inputs[0]) *\n",
    "    #                      (d_layer2_inputs[0] / d_W1[0])\n",
    "    #\n",
    "    # ...and so on for each node in Layer 2 and weight in W1\n",
    "    #\n",
    "    # So, we need to now find the values of each of the 3 partial derivatives in the equation above.\n",
    "    \n",
    "    # Calculate: d_ErrorT / d_layer2_outputs[0]\n",
    "    # \n",
    "    # The standard error calculation is:\n",
    "    # 0.5 * (correct_output - measured_output)^2\n",
    "    #\n",
    "    # ...and if we had multiple output nodes, then the total error would be\n",
    "    # the sum of the result of that equation over all output nodes.\n",
    "    # \n",
    "    # ErrorT = 0.5 * (Y - layer2_outputs[0])**2\n",
    "    \n",
    "    # Now, take the derivative of ErrorT with respect to the output.\n",
    "    # d_ErrorT / d_layer2_outputs[0] = 2 * 0.5 * (Y - layer2_outputs[0])**(2-1)\n",
    "    #                                = 1 * (Y - layer2_outputs[0]) * -1\n",
    "    #                                = -(Y - layer2_outputs[0])\n",
    "    d_ErrorT_d_layer_2_outputs = -(Y - layer2_outputs[0])\n",
    "    \n",
    "    # Next, we need to find how the output of each node in layer2 changes\n",
    "    # with respect to its input. Remember, we only have one output node.\n",
    "    #\n",
    "    # d_layer2_net_output / d_layer2_net_input\n",
    "    #\n",
    "    # The equation for the output is the sigmoid (aka logistic) function with the node's\n",
    "    # value as input. So, since our output is calculated as...\n",
    "    #\n",
    "    # layer2_outputs = sigmoid(layer2_inputs)\n",
    "    #\n",
    "    #  ...we will use the derivative of the sigmoid, which is output*(1-output):\n",
    "    #\n",
    "    # d_layer2_output / d_layer2_input = layer2_output * (1 - layer2_output)\n",
    "    d_layer2_outputs_d_layer2_inputs = layer2_outputs * (1 - layer2_outputs)\n",
    "    \n",
    "    # Finally, we need to calculate how much the input to Layer 2 changes with respect \n",
    "    # to each weight feeding into it.\n",
    "    # Again, the net input to the nodes in layer 2 are calculated by the sum of products\n",
    "    # from Layer 1 outputs and W1:\n",
    "    #\n",
    "    # layer2_net_input = layer1_outputs[0]*W1[0] +\n",
    "    #                    layer1_outputs[1]*W1[1] +\n",
    "    #                    layer1_outputs[2]*W1[2] +\n",
    "    #                    layer1_outputs[3]*W1[3]\n",
    "    #\n",
    "    # When we take the partial derivative of the above with respect to a given weight,\n",
    "    # the other terms all go to 0 since they're treated as constants, for example:\n",
    "    #\n",
    "    # d_layer2_net_input / d_layer1_weight[0] = 1*layer1_outputs[0]*W1[0]^(1-1) + 0 + 0 + 0\n",
    "    #                                         = layer1_outputs[0]\n",
    "    #\n",
    "    # Here, we need a loop becuase we have 4 partial derivatives to compute\n",
    "    d_layer2_inputs_d_layer1_weights = np.zeros((4,1))\n",
    "    for j in range(4):\n",
    "        d_layer2_inputs_d_layer1_weights[j][0] = layer1_outputs[0][j]\n",
    "        \n",
    "    # Now that we have all of our partial derivatives, we can multiply them together\n",
    "    # to find d_ErrorT / d_W1\n",
    "    d_errorT_d_W1 = d_ErrorT_d_layer_2_outputs * \\\n",
    "                    d_layer2_outputs_d_layer2_inputs * \\\n",
    "                    d_layer2_inputs_d_layer1_weights\n",
    "    \n",
    "    # Now we can calculate the deltas we want to apply to the weights in W1,\n",
    "    # scaled by our alpha parameter \n",
    "    W1_deltas = alpha * d_errorT_d_W1\n",
    "    \n",
    "    # Phew! That was a lot to get through, but now we've backpropagated the error\n",
    "    # in our output layer (Layer 2) to the hidden layer weights (W1)\n",
    "    #\n",
    "    # Now we get to do it all again for W0!\n",
    "    \n",
    "    # OK, so using the same logic, we will need to find how much changing each\n",
    "    # weight in W0 contributes to the total output error. For example, looking at\n",
    "    # the first weight in W0:\n",
    "    #\n",
    "    # d_ErrorT / d_W0[0] = (d_ErrorT / d_layer1_outputs[0]) * \n",
    "    #                      (d_layer1_outputs[0] / d_layer1_inputs[0]) *\n",
    "    #                      (d_layer1_inputs[0] / d_W0[0])\n",
    "    \n",
    "    # This time, we need to find the contribution of each hidden layer node to\n",
    "    # the total error, so we'll need a loop to cover all 4 hidden layer nodes.\n",
    "        \n",
    "    # First, find partial derivative of total error with respect to the\n",
    "    # hidden layer node's output.\n",
    "    #\n",
    "    # By the chain rule:\n",
    "    #\n",
    "    # d_ErrorT / d_layer1_outputs[0] = (d_ErrorT / d_layer2_outputs[0]) *\n",
    "    #                                  (d_layer2_outputs[0] / d_layer2_inputs[0]) *\n",
    "    #                                  (d_layer2_inputs[0] / d_layer1_outputs[0])\n",
    "    #\n",
    "    # The first two terms we know from earlier, so we only need to calculate the third term.\n",
    "    #\n",
    "    # layer2_inputs[0] is calculated as:\n",
    "    #\n",
    "    # layer2_inputs[0] = sum(layer1_outputs * W1) # aka dot product of layer1_outputs and W1\n",
    "    # layer2_inputs[0] = (layer1_outputs[0] * W1[0]) +\n",
    "    #                    (layer1_outputs[1] * W1[1]) +\n",
    "    #                    (layer1_outputs[2] * W1[2]) +\n",
    "    #                    (layer1_outputs[3] * W1[3])\n",
    "    #\n",
    "    # Then take the partial derivative with respect to layer1_outputs[0], which again\n",
    "    # is straightforward as all the other terms go to zero.\n",
    "    #\n",
    "    # d_layer2_inputs[0] / d_layer1_outputs[0] = W1[0]\n",
    "    d_layer2_inputs_d_layer1_outputs = W1.T\n",
    "    \n",
    "    # Next find the partial derivative of layer1_outputs with respect to layer1_inputs\n",
    "    #\n",
    "    # Layer 1 output is the sigmoid of the input\n",
    "    #\n",
    "    d_layer1_outputs_d_layer1_inputs = layer1_outputs*(1 - layer1_outputs)\n",
    "    \n",
    "    # And now find the partial derivative of layer1 inputs with respect to each W0.\n",
    "    #\n",
    "    # layer1_input[0] = (layer0[0] * W0[0][0]) +\n",
    "    #                   (layer0[1] * W0[1][0]) +\n",
    "    #                   (layer0[2] * W0[2][0])\n",
    "    #\n",
    "    # d_layer1_inputs[0] / d_W0[0][0] = layer0[0]\n",
    "    d_layer1_inputs_d_W0 = np.array([[0,0,1] for i in range(4)])\n",
    "    \n",
    "    # Finally, we can now determine how much each weight in w0 contributes to the total error:\n",
    "    d_errorT_d_W0 = d_ErrorT_d_layer_2_outputs * \\\n",
    "                    d_layer2_outputs_d_layer2_inputs * \\\n",
    "                    d_layer2_inputs_d_layer1_outputs * \\\n",
    "                    d_layer1_outputs_d_layer1_inputs * \\\n",
    "                    d_layer1_inputs_d_W0.T\n",
    " \n",
    "    # Now we can calculate the deltas we want to apply to the weights in W10,\n",
    "    # scaled by our alpha parameter \n",
    "    W0_deltas = alpha * d_errorT_d_W0\n",
    "    \n",
    "    # Update weights\n",
    "    W1 -= W1_deltas\n",
    "    W0 -= W0_deltas\n",
    "    \n",
    "    if (i% 5000 == 0):\n",
    "        print(\"Error: %.6f\" % np.mean(np.abs(Y-layer2_outputs)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
